{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5fc5e9-4639-48ad-afea-9617038bdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = set()\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and token.is_alpha:\n",
    "            tokens.add(token.lemma_)\n",
    "    return tokens\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "def process_resumes(folder_path):\n",
    "    resume_data = {}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.pdf'):\n",
    "            full_path = os.path.join(folder_path, fname)\n",
    "            text = extract_text_from_pdf(full_path)\n",
    "            tokens = tokenize(text)\n",
    "            resume_data[fname] = tokens\n",
    "    return resume_data\n",
    "\n",
    "def read_resumes_raw(base_dir, categories):\n",
    "    texts = []\n",
    "    names = []\n",
    "    for category in categories:\n",
    "        folder = os.path.join(base_dir, category)\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".pdf\"):\n",
    "                full_path = os.path.join(folder, fname)\n",
    "                text = extract_text_from_pdf(full_path)\n",
    "                texts.append(text)\n",
    "                names.append(fname)\n",
    "    return names, texts\n",
    "\n",
    "def process_job_descriptions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    jd_data = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        jd_text = str(row['Job Description'])\n",
    "        tokens = tokenize(jd_text)\n",
    "        jd_data[idx] = {\n",
    "            'job_title': row.get('Job Title', ''),\n",
    "            'tokens': tokens,\n",
    "            'full_desc': jd_text.strip()\n",
    "        }\n",
    "    return jd_data\n",
    "\n",
    "def read_job_descriptions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    jd_texts = df['Job Description'].astype(str).tolist()\n",
    "    jd_titles = df['Job Title'].astype(str).tolist()\n",
    "    return jd_titles, jd_texts\n",
    "\n",
    "def match_resumes_to_jobs(resume_tokens, jd_data, top_k=10):\n",
    "    results = defaultdict(list)\n",
    "    for resume_name, r_tokens in resume_tokens.items():\n",
    "        scores = []\n",
    "        for jd_id, jd_info in jd_data.items():\n",
    "            score = jaccard_similarity(r_tokens, jd_info['tokens'])\n",
    "            scores.append((jd_id, score, jd_info['job_title']))\n",
    "        top_matches = sorted(scores, key=lambda x: -x[1])[:top_k]\n",
    "        results[resume_name] = top_matches\n",
    "    return results\n",
    "\n",
    "def match_resumes_tfidf_cosine(resume_names, resume_texts, jd_titles, jd_texts, top_k=10):\n",
    "    all_docs = resume_texts + jd_texts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "    resume_vecs = tfidf_matrix[:len(resume_texts)]\n",
    "    jd_vecs = tfidf_matrix[len(resume_texts):]\n",
    "\n",
    "    sim_matrix = cosine_similarity(resume_vecs, jd_vecs)\n",
    "\n",
    "    results = {}\n",
    "    for i, resume_name in enumerate(resume_names):\n",
    "        top_matches = sorted(\n",
    "            list(enumerate(sim_matrix[i])), key=lambda x: -x[1]\n",
    "        )[:top_k]\n",
    "        results[resume_name] = [(idx, score, jd_titles[idx]) for idx, score in top_matches]\n",
    "    return results, vectorizer, jd_texts\n",
    "\n",
    "def match_resumes_sbert(resume_names, resume_texts, jd_titles, jd_texts, top_k=10):\n",
    "    # model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    resume_embeddings = model.encode(resume_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "    jd_embeddings = model.encode(jd_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    results = {}\n",
    "    for i, resume_name in enumerate(resume_names):\n",
    "        cosine_scores = util.cos_sim(resume_embeddings[i], jd_embeddings)[0]\n",
    "        top_results = torch.topk(cosine_scores, k=top_k)\n",
    "        top_matches = [(int(idx), float(cosine_scores[idx]), jd_titles[int(idx)]) for idx in top_results.indices]\n",
    "        results[resume_name] = top_matches\n",
    "    return results, model\n",
    "\n",
    "def compare_topk_overlap(resume_name, jaccard_results, tfidf_results):\n",
    "    jaccard_ids = {jd_id for jd_id, _, _ in jaccard_results.get(resume_name, [])}\n",
    "    tfidf_ids = {jd_id for jd_id, _, _ in tfidf_results.get(resume_name, [])}\n",
    "\n",
    "    overlap = jaccard_ids.intersection(tfidf_ids)\n",
    "\n",
    "    print(f\"\\nResume: {resume_name}\")\n",
    "    print(f\"Jaccard Top-K IDs: {sorted(jaccard_ids)}\")\n",
    "    print(f\"TF-IDF Top-K IDs: {sorted(tfidf_ids)}\")\n",
    "    print(f\"Overlap ({len(overlap)}): {sorted(overlap)}\")\n",
    "    print(f\"Jaccard-Only: {sorted(jaccard_ids - tfidf_ids)}\")\n",
    "    print(f\"TF-IDF-Only: {sorted(tfidf_ids - jaccard_ids)}\")\n",
    "\n",
    "def plot_score_distribution(jaccard_results, tfidf_results):\n",
    "    jaccard_scores = [score for matches in jaccard_results.values() for _, score, _ in matches]\n",
    "    tfidf_scores = [score for matches in tfidf_results.values() for _, score, _ in matches]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(jaccard_scores, bins=20, alpha=0.6, label='Jaccard', color='skyblue')\n",
    "    plt.hist(tfidf_scores, bins=20, alpha=0.6, label='TF-IDF', color='salmon')\n",
    "    plt.title(\"Similarity Score Distribution: Jaccard vs TF-IDF\")\n",
    "    plt.xlabel(\"Similarity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cc68ef-08fa-4482-9caf-7d0e1bfad67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['INFORMATION-TECHNOLOGY']\n",
    "base_resume_dir = './resume_pds/data/data'\n",
    "jd_csv_path = 'job_title_des.csv'\n",
    "\n",
    "resume_tokens = {}\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(base_resume_dir, category)\n",
    "    resume_tokens.update(process_resumes(folder_path))\n",
    "\n",
    "resume_names, resume_texts = read_resumes_raw(base_resume_dir, categories)\n",
    "jd_titles, jd_texts = read_job_descriptions(jd_csv_path)\n",
    "\n",
    "jd_data = process_job_descriptions(jd_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c859fdef-b9b0-4a85-b80f-3e017f5db429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_jaccard = match_resumes_to_jobs(resume_tokens, jd_data)\n",
    "# results_tfidf, vectorizer, all_jd_texts = match_resumes_tfidf_cosine(resume_names, resume_texts, jd_titles, jd_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6faebb70-24b0-4011-9804-32d84ec970c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 4/4 [00:03<00:00,  1.00it/s]\n",
      "Batches: 100%|██████████████████████████████████| 72/72 [00:54<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "results_sbert, sbert_model = match_resumes_sbert(resume_names, resume_texts, jd_titles, jd_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec560e2-b177-45a3-a9e5-3786ee4799ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top SBERT matches for 20674668.pdf:\n",
      "  JD ID: 128 | Title: JavaScript Developer... | SBERT Score: 0.781\n",
      "  JD ID: 940 | Title: JavaScript Developer... | SBERT Score: 0.771\n",
      "  JD ID: 1790 | Title: Java Developer... | SBERT Score: 0.762\n",
      "  JD ID: 1894 | Title: Backend Developer... | SBERT Score: 0.759\n",
      "  JD ID: 1466 | Title: Software Engineer... | SBERT Score: 0.758\n",
      "  JD ID: 487 | Title: Wordpress Developer... | SBERT Score: 0.754\n",
      "  JD ID: 1772 | Title: Full Stack Developer... | SBERT Score: 0.752\n",
      "  JD ID: 1425 | Title: PHP Developer... | SBERT Score: 0.750\n",
      "  JD ID: 2185 | Title: Wordpress Developer... | SBERT Score: 0.749\n",
      "  JD ID: 1519 | Title: Django Developer... | SBERT Score: 0.747\n"
     ]
    }
   ],
   "source": [
    "# Print top matches using SBERT for a specific resume\n",
    "target_resume = \"20674668.pdf\"\n",
    "print(f\"\\nTop SBERT matches for {target_resume}:\")\n",
    "for jd_id, score, title in results_sbert.get(target_resume, []):\n",
    "    print(f\"  JD ID: {jd_id} | Title: {title[:40]}... | SBERT Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6933fb40-a31b-48e5-9efd-b54356bd86ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 19.53it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# 1) Load your gold‐standard CSVs\n",
    "res_df = pd.read_csv('gold_resumes.csv')  # cols: Resume_ID, Resume_Text\n",
    "jd_df  = pd.read_csv('gold_jds.csv')      # cols: Job_ID,    Job_Text\n",
    "\n",
    "# 2) Prepare the inputs for SBERT matcher\n",
    "resume_names = res_df['Resume_ID'].astype(str).tolist()\n",
    "resume_texts = res_df['Resume_Text'].astype(str).tolist()\n",
    "jd_ids       = jd_df['Job_ID'].astype(str).tolist()\n",
    "jd_texts     = jd_df['Job_Text'].astype(str).tolist()\n",
    "\n",
    "# 3) Run your SBERT matcher on the gold data (top_k=4)\n",
    "results_sbert, sbert_model = match_resumes_sbert(\n",
    "    resume_names, resume_texts,\n",
    "    jd_ids, jd_texts,\n",
    "    top_k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7c73747-794f-43be-980d-e3d66e92c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = {\n",
    "    \"R1\": [\"J1\",\"J2\",\"J3\",\"J4\"],\n",
    "    \"R2\": [\"J5\",\"J6\",\"J7\",\"J8\"],\n",
    "    \"R3\": [\"J9\",\"J10\",\"J11\",\"J12\"],\n",
    "    \"R4\": [\"J13\",\"J14\",\"J15\",\"J16\"],\n",
    "    \"R5\": [\"J17\",\"J18\",\"J19\",\"J20\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8102224-0dd5-43f5-b5dc-dddaf0e5ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recs, gold_set, k):\n",
    "    return len(set(recs[:k]) & gold_set) / k\n",
    "\n",
    "def recall_at_k(recs, gold_set, k):\n",
    "    return len(set(recs[:k]) & gold_set) / len(gold_set)\n",
    "\n",
    "def topk_accuracy(recs, gold_set, k):\n",
    "    return 1.0 if set(recs[:k]) & gold_set else 0.0\n",
    "\n",
    "def reciprocal_rank(recs, gold_set):\n",
    "    for i, jid in enumerate(recs, start=1):\n",
    "        if jid in gold_set:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def dcg_at_k(rels, k):\n",
    "    return sum(r / math.log2(idx+1) for idx, r in enumerate(rels[:k], start=1))\n",
    "\n",
    "def ndcg_at_k(recs, gold_set, k):\n",
    "    rels  = [1 if jid in gold_set else 0 for jid in recs[:k]]\n",
    "    dcg   = dcg_at_k(rels, k)\n",
    "    ideal = [1]*min(len(gold_set), k)\n",
    "    idcg  = dcg_at_k(ideal, k)\n",
    "    return dcg / idcg if idcg>0 else 0.0\n",
    "\n",
    "def evaluate(results, gold, k=4):\n",
    "    P, R, T, RR, N = [], [], [], [], []\n",
    "    for rid, ranked in results.items():\n",
    "        if rid not in gold: \n",
    "            continue\n",
    "        gs   = set(gold[rid])\n",
    "        # ranked entries are (idx, score, job_id)\n",
    "        recs = [job_id for _, _, job_id in ranked]\n",
    "\n",
    "        P.append( precision_at_k(recs, gs, k) )\n",
    "        R.append( recall_at_k(recs, gs, k)    )\n",
    "        T.append( topk_accuracy(recs, gs, k)  )\n",
    "        RR.append(reciprocal_rank(recs, gs)   )\n",
    "        N.append( ndcg_at_k(recs, gs, k)      )\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\":   sum(P)/len(P),\n",
    "        f\"Recall@{k}\":      sum(R)/len(R),\n",
    "        f\"Top-{k} Acc.\":    sum(T)/len(T),\n",
    "        \"MRR\":              sum(RR)/len(RR),\n",
    "        f\"NDCG@{k}\":        sum(N)/len(N),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850a3c4f-227d-4052-8a78-23c863242be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SBERT Evaluation on Gold Data ===\n",
      "Precision@4: 0.700\n",
      "Recall@4: 0.700\n",
      "Top-4 Acc.: 1.000\n",
      "MRR: 0.900\n",
      "NDCG@4: 0.733\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(results_sbert, gold, k=4)\n",
    "print(\"=== SBERT Evaluation on Gold Data ===\")\n",
    "for name, val in metrics.items():\n",
    "    print(f\"{name}: {val:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
