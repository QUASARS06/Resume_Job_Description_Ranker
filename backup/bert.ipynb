{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5fc5e9-4639-48ad-afea-9617038bdf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiragjain/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = set()\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and token.is_alpha:\n",
    "            tokens.add(token.lemma_)\n",
    "    return tokens\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "def process_resumes(folder_path):\n",
    "    resume_data = {}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.pdf'):\n",
    "            full_path = os.path.join(folder_path, fname)\n",
    "            text = extract_text_from_pdf(full_path)\n",
    "            tokens = tokenize(text)\n",
    "            resume_data[fname] = tokens\n",
    "    return resume_data\n",
    "\n",
    "def read_resumes_raw(base_dir, categories):\n",
    "    texts = []\n",
    "    names = []\n",
    "    for category in categories:\n",
    "        folder = os.path.join(base_dir, category)\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".pdf\"):\n",
    "                full_path = os.path.join(folder, fname)\n",
    "                text = extract_text_from_pdf(full_path)\n",
    "                texts.append(text)\n",
    "                names.append(fname)\n",
    "    return names, texts\n",
    "\n",
    "def process_job_descriptions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    jd_data = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        jd_text = str(row['Job Description'])\n",
    "        tokens = tokenize(jd_text)\n",
    "        jd_data[idx] = {\n",
    "            'job_title': row.get('Job Title', ''),\n",
    "            'tokens': tokens,\n",
    "            'full_desc': jd_text.strip()\n",
    "        }\n",
    "    return jd_data\n",
    "\n",
    "def read_job_descriptions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    jd_texts = df['Job Description'].astype(str).tolist()\n",
    "    jd_titles = df['Job Title'].astype(str).tolist()\n",
    "    return jd_titles, jd_texts\n",
    "\n",
    "def match_resumes_to_jobs(resume_tokens, jd_data, top_k=10):\n",
    "    results = defaultdict(list)\n",
    "    for resume_name, r_tokens in resume_tokens.items():\n",
    "        scores = []\n",
    "        for jd_id, jd_info in jd_data.items():\n",
    "            score = jaccard_similarity(r_tokens, jd_info['tokens'])\n",
    "            scores.append((jd_id, score, jd_info['job_title']))\n",
    "        top_matches = sorted(scores, key=lambda x: -x[1])[:top_k]\n",
    "        results[resume_name] = top_matches\n",
    "    return results\n",
    "\n",
    "def match_resumes_tfidf_cosine(resume_names, resume_texts, jd_titles, jd_texts, top_k=10):\n",
    "    all_docs = resume_texts + jd_texts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "    resume_vecs = tfidf_matrix[:len(resume_texts)]\n",
    "    jd_vecs = tfidf_matrix[len(resume_texts):]\n",
    "\n",
    "    sim_matrix = cosine_similarity(resume_vecs, jd_vecs)\n",
    "\n",
    "    results = {}\n",
    "    for i, resume_name in enumerate(resume_names):\n",
    "        top_matches = sorted(\n",
    "            list(enumerate(sim_matrix[i])), key=lambda x: -x[1]\n",
    "        )[:top_k]\n",
    "        results[resume_name] = [(idx, score, jd_titles[idx]) for idx, score in top_matches]\n",
    "    return results, vectorizer, jd_texts\n",
    "\n",
    "def match_resumes_sbert(resume_names, resume_texts, jd_titles, jd_texts, top_k=10):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    resume_embeddings = model.encode(resume_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "    jd_embeddings = model.encode(jd_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    results = {}\n",
    "    for i, resume_name in enumerate(resume_names):\n",
    "        cosine_scores = util.cos_sim(resume_embeddings[i], jd_embeddings)[0]\n",
    "        top_results = torch.topk(cosine_scores, k=top_k)\n",
    "        top_matches = [(int(idx), float(cosine_scores[idx]), jd_titles[int(idx)]) for idx in top_results.indices]\n",
    "        results[resume_name] = top_matches\n",
    "    return results, model\n",
    "\n",
    "def compare_topk_overlap(resume_name, jaccard_results, tfidf_results):\n",
    "    jaccard_ids = {jd_id for jd_id, _, _ in jaccard_results.get(resume_name, [])}\n",
    "    tfidf_ids = {jd_id for jd_id, _, _ in tfidf_results.get(resume_name, [])}\n",
    "\n",
    "    overlap = jaccard_ids.intersection(tfidf_ids)\n",
    "\n",
    "    print(f\"\\nResume: {resume_name}\")\n",
    "    print(f\"Jaccard Top-K IDs: {sorted(jaccard_ids)}\")\n",
    "    print(f\"TF-IDF Top-K IDs: {sorted(tfidf_ids)}\")\n",
    "    print(f\"Overlap ({len(overlap)}): {sorted(overlap)}\")\n",
    "    print(f\"Jaccard-Only: {sorted(jaccard_ids - tfidf_ids)}\")\n",
    "    print(f\"TF-IDF-Only: {sorted(tfidf_ids - jaccard_ids)}\")\n",
    "\n",
    "def plot_score_distribution(jaccard_results, tfidf_results):\n",
    "    jaccard_scores = [score for matches in jaccard_results.values() for _, score, _ in matches]\n",
    "    tfidf_scores = [score for matches in tfidf_results.values() for _, score, _ in matches]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(jaccard_scores, bins=20, alpha=0.6, label='Jaccard', color='skyblue')\n",
    "    plt.hist(tfidf_scores, bins=20, alpha=0.6, label='TF-IDF', color='salmon')\n",
    "    plt.title(\"Similarity Score Distribution: Jaccard vs TF-IDF\")\n",
    "    plt.xlabel(\"Similarity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cc68ef-08fa-4482-9caf-7d0e1bfad67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.62it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top SBERT matches for 20674668.pdf:\n",
      "  JD ID: 1088 | Title: PHP Developer... | SBERT Score: 0.728\n",
      "  JD ID: 754 | Title: Node js developer... | SBERT Score: 0.728\n",
      "  JD ID: 611 | Title: Node js developer... | SBERT Score: 0.713\n",
      "  JD ID: 1673 | Title: Wordpress Developer... | SBERT Score: 0.703\n",
      "  JD ID: 1214 | Title: PHP Developer... | SBERT Score: 0.702\n",
      "  JD ID: 265 | Title: Node js developer... | SBERT Score: 0.700\n",
      "  JD ID: 940 | Title: JavaScript Developer... | SBERT Score: 0.699\n",
      "  JD ID: 97 | Title: Wordpress Developer... | SBERT Score: 0.697\n",
      "  JD ID: 2213 | Title: Node js developer... | SBERT Score: 0.695\n",
      "  JD ID: 2154 | Title: JavaScript Developer... | SBERT Score: 0.694\n"
     ]
    }
   ],
   "source": [
    "categories = ['INFORMATION-TECHNOLOGY']\n",
    "base_resume_dir = './resume_pds/data/data'\n",
    "jd_csv_path = 'job_title_des.csv'\n",
    "\n",
    "resume_tokens = {}\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(base_resume_dir, category)\n",
    "    resume_tokens.update(process_resumes(folder_path))\n",
    "\n",
    "resume_names, resume_texts = read_resumes_raw(base_resume_dir, categories)\n",
    "jd_titles, jd_texts = read_job_descriptions(jd_csv_path)\n",
    "\n",
    "jd_data = process_job_descriptions(jd_csv_path)\n",
    "results_jaccard = match_resumes_to_jobs(resume_tokens, jd_data)\n",
    "results_tfidf, vectorizer, all_jd_texts = match_resumes_tfidf_cosine(resume_names, resume_texts, jd_titles, jd_texts)\n",
    "results_sbert, sbert_model = match_resumes_sbert(resume_names, resume_texts, jd_titles, jd_texts)\n",
    "\n",
    "# Print top matches using SBERT for a specific resume\n",
    "target_resume = \"20674668.pdf\"\n",
    "print(f\"\\nTop SBERT matches for {target_resume}:\")\n",
    "for jd_id, score, title in results_sbert.get(target_resume, []):\n",
    "    print(f\"  JD ID: {jd_id} | Title: {title[:40]}... | SBERT Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859fdef-b9b0-4a85-b80f-3e017f5db429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
